{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Process Train Data\n",
        "\n",
        "This notebook processes the training data and separates labels from features.\n",
        "\n",
        "## Steps:\n",
        "1. Load training data from `data/external/train.parquet`\n",
        "2. Identify the target column (label)\n",
        "3. Separate features (X) and labels (y)\n",
        "4. Save processed data to `data/processed/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up paths\n",
        "data_dir = project_root / \"data\"\n",
        "external_dir = data_dir / \"external\"\n",
        "processed_dir = data_dir / \"processed\"\n",
        "\n",
        "train_path = external_dir / \"train.parquet\"\n",
        "\n",
        "print(f\"Loading data from: {train_path}\")\n",
        "print(f\"File exists: {train_path.exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "print(\"Loading training data...\")\n",
        "df = pd.read_parquet(train_path)\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore Data Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display column names\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(df.columns.tolist()[:20])  # Show first 20 columns\n",
        "if len(df.columns) > 20:\n",
        "    print(f\"... and {len(df.columns) - 20} more columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display data types\n",
        "print(\"Data types:\")\n",
        "print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify Target Column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common target column names in Kaggle competitions\n",
        "possible_targets = ['target', 'default', 'label', 'y']\n",
        "\n",
        "target_col = None\n",
        "for col in possible_targets:\n",
        "    if col in df.columns:\n",
        "        target_col = col\n",
        "        print(f\"Found target column: '{col}'\")\n",
        "        break\n",
        "\n",
        "if target_col is None:\n",
        "    print(\"Could not find common target column names.\")\n",
        "    print(\"Checking for binary columns...\")\n",
        "    \n",
        "    # Check for binary columns (0/1)\n",
        "    binary_cols = []\n",
        "    for col in df.columns:\n",
        "        unique_vals = df[col].unique()\n",
        "        if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n",
        "            binary_cols.append(col)\n",
        "    \n",
        "    if len(binary_cols) == 1:\n",
        "        target_col = binary_cols[0]\n",
        "        print(f\"Found binary column as target: '{target_col}'\")\n",
        "    else:\n",
        "        print(f\"Found {len(binary_cols)} binary columns: {binary_cols}\")\n",
        "        print(\"Please manually specify the target column.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If target column was not automatically identified, specify it here\n",
        "# target_col = 'target'  # Uncomment and set the correct column name\n",
        "\n",
        "if target_col:\n",
        "    print(f\"\\nTarget column: '{target_col}'\")\n",
        "    print(f\"\\nTarget value counts:\")\n",
        "    print(df[target_col].value_counts().sort_index())\n",
        "    print(f\"\\nTarget distribution:\")\n",
        "    print(df[target_col].value_counts(normalize=True).sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Separate Features and Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get customer ID if it exists (usually first column or named 'customer_ID')\n",
        "id_col = None\n",
        "if 'customer_ID' in df.columns:\n",
        "    id_col = 'customer_ID'\n",
        "elif 'id' in df.columns:\n",
        "    id_col = 'id'\n",
        "elif 'customer_id' in df.columns:\n",
        "    id_col = 'customer_id'\n",
        "\n",
        "if id_col:\n",
        "    print(f\"Found ID column: '{id_col}'\")\n",
        "else:\n",
        "    print(\"No ID column found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate target (labels)\n",
        "y = df[target_col].copy()\n",
        "\n",
        "# Separate features (exclude target and ID columns)\n",
        "exclude_cols = [target_col]\n",
        "if id_col:\n",
        "    exclude_cols.append(id_col)\n",
        "\n",
        "X = df.drop(columns=exclude_cols).copy()\n",
        "\n",
        "# Store IDs separately if they exist\n",
        "ids = df[id_col].copy() if id_col else None\n",
        "\n",
        "print(f\"Features (X) shape: {X.shape}\")\n",
        "print(f\"Labels (y) shape: {y.shape}\")\n",
        "if ids is not None:\n",
        "    print(f\"IDs shape: {ids.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"FEATURES SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nShape: {X.shape}\")\n",
        "print(f\"Columns: {len(X.columns)}\")\n",
        "print(f\"Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"\\nData types:\")\n",
        "print(X.dtypes.value_counts())\n",
        "print(f\"\\nMissing values: {X.isnull().sum().sum()}\")\n",
        "if X.isnull().sum().sum() > 0:\n",
        "    print(f\"Columns with missing values: {X.isnull().sum()[X.isnull().sum() > 0].shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"LABELS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nShape: {y.shape}\")\n",
        "print(f\"\\nValue counts:\")\n",
        "print(y.value_counts().sort_index())\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(y.value_counts(normalize=True).sort_index())\n",
        "print(f\"\\nData type: {y.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ids is not None:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CUSTOMER IDs SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nShape: {ids.shape}\")\n",
        "    print(f\"Unique IDs: {ids.nunique()}\")\n",
        "    print(f\"Data type: {ids.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create processed directory if it doesn't exist\n",
        "processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Saving processed data to: {processed_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save features\n",
        "features_path = processed_dir / \"X_train.parquet\"\n",
        "X.to_parquet(features_path, index=False, compression='snappy')\n",
        "print(f\"✓ Features saved: {features_path}\")\n",
        "print(f\"  Shape: {X.shape}\")\n",
        "print(f\"  Size: {features_path.stat().st_size / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save labels\n",
        "labels_path = processed_dir / \"y_train.parquet\"\n",
        "y.to_frame().to_parquet(labels_path, index=False, compression='snappy')\n",
        "print(f\"✓ Labels saved: {labels_path}\")\n",
        "print(f\"  Shape: {y.shape}\")\n",
        "print(f\"  Size: {labels_path.stat().st_size / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save IDs if they exist\n",
        "if ids is not None:\n",
        "    ids_path = processed_dir / \"train_ids.parquet\"\n",
        "    ids.to_frame().to_parquet(ids_path, index=False, compression='snappy')\n",
        "    print(f\"✓ IDs saved: {ids_path}\")\n",
        "    print(f\"  Shape: {ids.shape}\")\n",
        "    print(f\"  Size: {ids_path.stat().st_size / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify saved files can be loaded\n",
        "print(\"Verifying saved files...\")\n",
        "\n",
        "X_loaded = pd.read_parquet(features_path)\n",
        "y_loaded = pd.read_parquet(labels_path)\n",
        "\n",
        "print(f\"✓ Features loaded: {X_loaded.shape}\")\n",
        "print(f\"✓ Labels loaded: {y_loaded.shape}\")\n",
        "\n",
        "# Check if shapes match\n",
        "assert X_loaded.shape == X.shape, \"Features shape mismatch!\"\n",
        "assert y_loaded.shape[0] == y.shape[0], \"Labels shape mismatch!\"\n",
        "\n",
        "print(\"\\n✓ All verifications passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The training data has been successfully processed:\n",
        "- Features (X) saved to: `data/processed/X_train.parquet`\n",
        "- Labels (y) saved to: `data/processed/y_train.parquet`\n",
        "- Customer IDs (if available) saved to: `data/processed/train_ids.parquet`\n",
        "\n",
        "You can now use these processed files for model training.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
